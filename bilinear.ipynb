{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTATION BILINEAR ATTENTION FOR SHIFT NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((256, 32, 32)).cuda()\n",
    "y = torch.rand((256, 32, 32)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 1\n",
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unfold(img, patch_size, stride, with_indexes=False):\n",
    "    n_dim = 3\n",
    "    assert img.dim() == n_dim, 'image must be of dimension 3.'\n",
    "\n",
    "    kH, kW = patch_size, patch_size\n",
    "    dH, dW = stride, stride\n",
    "    input_windows = img.unfold(1, kH, dH).unfold(2, kW, dW)\n",
    "\n",
    "    i_1, i_2, i_3, i_4, i_5 = input_windows.size(0), input_windows.size(1), input_windows.size(\n",
    "        2), input_windows.size(3), input_windows.size(4)\n",
    "\n",
    "    if with_indexes:\n",
    "        input_windows = input_windows.permute(1, 2, 0, 3, 4).contiguous().view(i_2 * i_3, i_1)\n",
    "        return input_windows, i_2, i_3, i_1, i_4\n",
    "    else:\n",
    "        input_windows = input_windows.permute(1, 2, 0, 3, 4).contiguous().view(i_2 * i_3, i_1, i_4, i_5)\n",
    "    return input_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unfolded = _unfold(x, patch_size, stride)\n",
    "y_unfolded = _unfold(y, patch_size, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _filter(input_windows, flag, value):\n",
    "    ## EXTRACT MASK OR NOT DEPENDING ON VALUE\n",
    "    input_window = input_windows[flag == value]\n",
    "    return input_window.view(input_window.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = torch.rand((1024)) > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unfolded_filtered = _filter(x_unfolded, flag, 1)\n",
    "y_unfolded_filtered = _filter(y_unfolded, flag, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([264, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unfolded_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([760, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_unfolded_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_var(x, y, dim=1024, _v=1):\n",
    "    N = K = x.size(0)\n",
    "    M = y.size(0)\n",
    "    # VERSION 1, REAL IMPLEMENTATION\n",
    "    if _v == 0:\n",
    "        U = torch.randn((N, K)).cuda()\n",
    "        V = torch.randn((M, K)).cuda()\n",
    "    # VERSION 2, FOR ME :)\n",
    "    else:\n",
    "        U = torch.randn((dim, dim)).cuda()\n",
    "        V = torch.randn((dim, dim)).cuda()\n",
    "\n",
    "    return U, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 484 ms\n"
     ]
    }
   ],
   "source": [
    "%time U, V = create_var(x_unfolded_filtered, y_unfolded_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BAN(x, y, U, V):\n",
    "    def _bilinear_attention_map(x, y, U, V, v1, P):\n",
    "        print(v1.shape, P.shape, 'i,j->ij')\n",
    "        tmp = torch.einsum('i,j->ij', v1, P)\n",
    "        print(tmp.shape)\n",
    "        print()\n",
    "        \n",
    "        print(x.shape, U.shape, 'ki,kj->ij')\n",
    "        XT_U = torch.einsum('ki,kj->ij', [x, U])\n",
    "        print(XT_U.shape)\n",
    "        print()\n",
    "        \n",
    "        print(tmp.shape, XT_U.shape, 'ij,ij->ij')\n",
    "        tmp1 = torch.einsum('ij,ij->ij', [tmp, XT_U])\n",
    "        print(tmp1.shape)\n",
    "        print()\n",
    "        \n",
    "        print(V.shape, y.shape, 'ki,kj->ij')\n",
    "        VT_y = torch.einsum('ki,kj->ij', [V, y])\n",
    "        print(VT_y.shape)\n",
    "        print()\n",
    "        \n",
    "        print(tmp1.shape, VT_y.shape, 'ik,kj->ij')\n",
    "        tmp = torch.einsum('ik,kj->ij', [tmp1, VT_y])\n",
    "        print(tmp.shape)\n",
    "        print()\n",
    "        \n",
    "        tmp /= tmp.max()\n",
    "        \n",
    "        A = F.softmax(tmp, dim=1)   \n",
    "        print(A.shape)\n",
    "        print()\n",
    "        return A, XT_U, VT_y\n",
    "    \n",
    "    def _filter_M(U,V,flag):\n",
    "        mask_indexes = (flag == 1).nonzero().t()\n",
    "        non_mask_indexes = (flag == 0).nonzero()\n",
    "        U = U[mask_indexes.t(), mask_indexes]\n",
    "        V = V[non_mask_indexes, mask_indexes]\n",
    "        return U, V\n",
    "    \n",
    "    def _emulate(x, y):\n",
    "        flag = torch.rand((1024)) > 0.75\n",
    "        x_unfolded = _unfold(x, patch_size, stride)\n",
    "        y_unfolded = _unfold(y, patch_size, stride)\n",
    "        X = _filter(x_unfolded, flag, 1)\n",
    "        Y = _filter(y_unfolded, flag, 0)  \n",
    "        \n",
    "        N = K = X.size(0)\n",
    "        M = Y.size(0)\n",
    "        p = X.size(1)\n",
    "        r = Y.size(1)\n",
    "        \n",
    "        P = torch.randn((K)).cuda()\n",
    "        v1 = torch.randn((p)).cuda()\n",
    "        return X, Y, P, v1, flag\n",
    "\n",
    "    ##EMULATE NEW MASK GENERATED\n",
    "    print('EMULATE')\n",
    "    X, Y, P, v1, flag = _emulate(x, y)\n",
    "    U, V = _filter_M(U, V, flag)\n",
    "    print('EMULATE')\n",
    "    print()\n",
    "    \n",
    "    A, XT_U, VT_y = _bilinear_attention_map(X, Y, U, V, v1, P) \n",
    "    print(A.shape, XT_U.shape, VT_y.shape, 'ki,kk,jk->i')\n",
    "    A = torch.einsum('ki,kk,jk->i', [XT_U, A, VT_y])\n",
    "    print(A.shape, v1.shape, 'i,j->ij')\n",
    "    A = torch.einsum('i,j->ij', [A, v1])\n",
    "    #A /= A.max()\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMULATE\n",
      "EMULATE\n",
      "\n",
      "torch.Size([256]) torch.Size([259]) i,j->ij\n",
      "torch.Size([256, 259])\n",
      "\n",
      "torch.Size([259, 256]) torch.Size([259, 259]) ki,kj->ij\n",
      "torch.Size([256, 259])\n",
      "\n",
      "torch.Size([256, 259]) torch.Size([256, 259]) ij,ij->ij\n",
      "torch.Size([256, 259])\n",
      "\n",
      "torch.Size([765, 259]) torch.Size([765, 256]) ki,kj->ij\n",
      "torch.Size([259, 256])\n",
      "\n",
      "torch.Size([256, 259]) torch.Size([259, 256]) ik,kj->ij\n",
      "torch.Size([256, 256])\n",
      "\n",
      "torch.Size([256, 256])\n",
      "\n",
      "torch.Size([256, 256]) torch.Size([256, 259]) torch.Size([259, 256]) ki,kk,jk->i\n",
      "torch.Size([259]) torch.Size([256]) i,j->ij\n",
      "Wall time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "%time out = BAN(x, y, U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT THE PRINT TO PERFORM CORRECT TIME BENCHMARK\n",
    "def BAN(x, y, U, V):\n",
    "    def _bilinear_attention_map(x, y, U, V, v1, P):\n",
    "        tmp = torch.einsum('i,j->ij', v1, P)\n",
    "        XT_U = torch.einsum('ki,kj->ij', [x, U])\n",
    "        tmp1 = torch.einsum('ij,ij->ij', [tmp, XT_U])\n",
    "        VT_y = torch.einsum('ki,kj->ij', [V, y])\n",
    "        tmp = torch.einsum('ik,kj->ij', [tmp1, VT_y])\n",
    "        tmp /= tmp.max()\n",
    "        A = F.softmax(tmp, dim=1)   \n",
    "        return A, XT_U, VT_y\n",
    "    \n",
    "    def _filter_M(U, V, flag):\n",
    "        mask_indexes = (flag == 1).nonzero().t()\n",
    "        non_mask_indexes = (flag == 0).nonzero()\n",
    "        U = U[mask_indexes.t(), mask_indexes]\n",
    "        V = V[non_mask_indexes, mask_indexes]\n",
    "        return U, V\n",
    "    \n",
    "    def _emulate(x, y, U, V):\n",
    "        flag = torch.rand((1024)) > 0.75\n",
    "        x_unfolded = _unfold(x, patch_size, stride)\n",
    "        y_unfolded = _unfold(y, patch_size, stride)\n",
    "        X = _filter(x_unfolded, flag, 1)\n",
    "        Y = _filter(y_unfolded, flag, 0)  \n",
    "        \n",
    "        N = K = X.size(0)\n",
    "        M = Y.size(0)\n",
    "        p = X.size(1)\n",
    "        r = Y.size(1)\n",
    "        \n",
    "        P = torch.randn((K)).cuda()\n",
    "        v1 = torch.randn((p)).cuda()\n",
    "        U, V = _filter_M(U, V, flag)\n",
    "        return X, Y, P, v1, flag, U, V\n",
    "\n",
    "    X, Y, P, v1, flag, u, v = _emulate(x, y, U, V)\n",
    "    \n",
    "    A, XT_U, VT_y = _bilinear_attention_map(X, Y, u, v, v1, P) \n",
    "    A = torch.einsum('ki,kk,jk->i', [XT_U, A, VT_y])\n",
    "    A = torch.einsum('i,j->ij', [A, v1])\n",
    "    #A /= A.max()\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.63 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit BAN(x, y, U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([259, 256])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -779.3818,   213.2228,  -803.4930,  ...,  -116.9878,   656.7700,\n",
       "           198.3965],\n",
       "        [-1502.6038,   411.0814, -1549.0889,  ...,  -225.5457,  1266.2153,\n",
       "           382.4972],\n",
       "        [ 1524.9980,  -417.2081,  1572.1760,  ...,   228.9072, -1285.0865,\n",
       "          -388.1978],\n",
       "        ...,\n",
       "        [ 2398.3250,  -656.1323,  2472.5205,  ...,   359.9965, -2021.0225,\n",
       "          -610.5086],\n",
       "        [-2057.4216,   562.8682, -2121.0708,  ...,  -308.8257,  1733.7496,\n",
       "           523.7295],\n",
       "        [ -151.3851,    41.4158,  -156.0684,  ...,   -22.7234,   127.5693,\n",
       "            38.5360]], device='cuda:0')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOST OF THE TIMES WAS SPENT IN CREATING THE BIG U AND V MATRICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
